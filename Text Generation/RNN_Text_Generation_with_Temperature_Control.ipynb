{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# You can replace this with a larger text file (e.g., a Shakespeare corpus)\n",
        "# For a quick runnable example, we'll use a small built-in text.\n",
        "# To use a file, uncomment the lines below and provide the path.\n",
        "# FILE_PATH = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# with open(FILE_PATH, 'r') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "# Small sample text for demonstration\n",
        "text = \"\"\"\n",
        "All the world's a stage,\n",
        "And all the men and women merely players;\n",
        "They have their exits and their entrances;\n",
        "And one man in his time plays many parts,\n",
        "His acts being seven ages. At first the infant,\n",
        "Mewling and puking in the nurse's arms;\n",
        "Then the whining school-boy, with his satchel\n",
        "And shining morning face, creeping like snail\n",
        "Unwillingly to school. And then the lover,\n",
        "Sighing like furnace, with a woeful ballad\n",
        "Made to his mistress' eyebrow. Then a soldier,\n",
        "Full of strange oaths and bearded like the pard,\n",
        "Jealous in honour, sudden and quick in quarrel,\n",
        "Seeking the bubble reputation\n",
        "Even in the cannon's mouth. And then the justice,\n",
        "In fair round belly with good capon lined,\n",
        "With eyes severe and beard of formal cut,\n",
        "Full of wise saws and modern instances;\n",
        "And so he plays his part. The sixth age shifts\n",
        "Into the lean and slipper'd pantaloon,\n",
        "With spectacles on nose and pouch on side,\n",
        "His youthful hose, well saved, a world too wide\n",
        "For his shrunk shank; and his big manly voice,\n",
        "Turning again toward childish treble, pipes\n",
        "And whistles in his sound. Last scene of all,\n",
        "That ends this strange eventful history,\n",
        "Is second childishness and mere oblivion,\n",
        "Sans teeth, sans eyes, sans taste, sans everything.\n",
        "\"\"\"\n",
        "\n",
        "# Convert all text to lowercase for consistency\n",
        "text = text.lower()\n",
        "\n",
        "# --- Data Preprocessing ---\n",
        "# Create a vocabulary of unique characters\n",
        "vocab = sorted(list(set(text)))\n",
        "char_to_int = {char: i for i, char in enumerate(vocab)}\n",
        "int_to_char = {i: char for i, char in enumerate(vocab)}\n",
        "\n",
        "# Print vocabulary size\n",
        "print(f\"Vocabulary size: {len(vocab)} unique characters.\")\n",
        "\n",
        "# Convert text to integers\n",
        "encoded_text = [char_to_int[char] for char in text]\n",
        "\n",
        "# Create training sequences\n",
        "seq_length = 100  # Length of input sequences\n",
        "examples_per_epoch = len(encoded_text) // (seq_length + 1)\n",
        "\n",
        "# Create character datasets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "# Batch sequences for training\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch and shuffle the dataset\n",
        "batch_size = 64\n",
        "buffer_size = 10000  # TF data will prefetch this many elements to disk\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "model.summary()\n",
        "\n",
        "# --- Training ---\n",
        "# Loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Configure checkpoints to save model weights during training\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "epochs = 20  # You might need more epochs for a larger dataset\n",
        "history = model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])\n",
        "\n",
        "# --- Text Generation Function ---\n",
        "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained Keras model.\n",
        "        start_string: The initial string to start generation from.\n",
        "        num_generate: The number of characters to generate.\n",
        "        temperature: A float value, controls the randomness of predictions.\n",
        "                     Higher temperature (e.g., 1.0) means more creative/random.\n",
        "                     Lower temperature (e.g., 0.2) means more conservative/predictable.\n",
        "                     Temperature close to 0 makes it almost greedy (picks the highest probability).\n",
        "    Returns:\n",
        "        The generated text.\n",
        "    \"\"\"\n",
        "    # Convert start string to numbers (integers)\n",
        "    input_eval = [char_to_int[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0) # Add batch dimension\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Reset the model's state before starting generation\n",
        "    # This is important for stateful RNNs\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Apply temperature to logits\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample a character from the distribution\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(int_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# --- User Interaction ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Text Generation ---\")\n",
        "    print(\"Enter a starting string (e.g., 'romeo and juliet'):\")\n",
        "    seed_text = input().lower() # Convert input to lowercase\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"Enter a temperature value (e.g., 0.5 for less random, 1.0 for more random, 1.5 for very creative):\")\n",
        "            temperature_input = float(input())\n",
        "            if temperature_input <= 0:\n",
        "                print(\"Temperature must be greater than 0. Setting to 0.1.\")\n",
        "                temperature_input = 0.1\n",
        "            break\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a number for temperature.\")\n",
        "\n",
        "    print(\"\\nGenerating text...\")\n",
        "    generated_text = generate_text(model, start_string=seed_text, num_generate=500, temperature=temperature_input)\n",
        "    print(\"\\n--- Generated Text ---\")\n",
        "    print(generated_text)\n",
        "    print(\"\\n--- End of Generation ---\")\n",
        "\n",
        "    # You can also load the latest checkpoint and generate text\n",
        "    # latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    # model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) # For inference, batch_size is 1\n",
        "    # model.load_weights(latest_checkpoint)\n",
        "    # model.build(tf.TensorShape([1, None]))\n",
        "    # print(\"\\n--- Generated Text from Loaded Model ---\")\n",
        "    # print(generate_text(model, start_string=seed_text, num_generate=500, temperature=temperature_input))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32 unique characters.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [64, None]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3588853744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mrnn_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3588853744.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(vocab_size, embedding_dim, rnn_units, batch_size)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     model = tf.keras.Sequential([\n\u001b[0;32m---> 85\u001b[0;31m         tf.keras.layers.Embedding(vocab_size, embedding_dim,\n\u001b[0m\u001b[1;32m     86\u001b[0m                                   batch_input_shape=[batch_size, None]),\n\u001b[1;32m     87\u001b[0m         tf.keras.layers.GRU(rnn_units,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, weights, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;34m\"Argument `input_length` is deprecated. Just remove it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             )\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_shape_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0;34m\"Unrecognized keyword arguments \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;34mf\"passed to {self.__class__.__name__}: {kwargs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [64, None]}"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "DFkRFf9vsWO-",
        "outputId": "e07a3dd6-74d1-4559-de1f-7810dc3d8e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbpQ7RQdzY-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}